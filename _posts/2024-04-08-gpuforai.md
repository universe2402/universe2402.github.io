---
layout: post
title: AI 모델 추론을 위한 gpu 스펙 비교
# author: universe
description: null
tags: [pytorch, gpu]
featuredImage: null
img: null
categories: [llm]
date: "2024-04-08"
---

## 잡담 == skip 가능

AI모델들의 파라미터수가 많아지고, huggingface라는 플랫폼을 통해 기본 모델들을 다운받아 finetuning하는 형태로 사용하는 패턴이 보편화되면서, 실제 사용에 있어서 대부분의 사람들과 기업들에게 있어서 점점 모델 학습보다는 모델 추론에 관심이 더 많아지고 있습니다. 이런 트랜드를 nvidia에서도 눈치를 챘는지, 새로운 gpu 모델들은 이런 추론 성능을 높이는데 좀 더 집중하고, 이 부분에 초점을 맞춰서 홍보를 하고 있습니다.

당연하게도 최신 gpu를 사용하는것이 성능을 높이는데 가장 좋은 선택이지만, 돈이 없거나, 미리 주문을 못했거나, gpu가 부족해서 이전에 사둔 gpu도 써야되거나 등등 다양한 이유로 최신 gpu를 사용하지 못하는 경우에는 안타깝게도 예전 버전의 gpu를 사용해야하는데, 이런 경우 최신 기술들이 지원되지 않는다거나, 메모리 용량이 부족하거나 bandwidth가 낮아서 bottleneck이 발생하거나 등등의 제약이 발생합니다.

## gpu 스펙 비교

이에 대표적인 서버용 gpu들의 스펙을 나열해보고 발생 가능한 제약 및 이를 극복하는 방법에 대해서 소개를 하고자 합니다. 결론적으로는 적어도 현시점 기준 최소 A100은 있어야 큰 제약없이 사용할 수 있습니다. 더불어 필요한 스펙들은 transformer 기반의 모델을 기준으로 작성되었습니다.

**[table info supported by google gemini]**

|                     | V100    | A100         | H100     | H200     |
| ------------------- | ------- | ------------ | -------- | -------- |
| 출시일(년도)        | 2017    | 2020         | 2022     | 2023     |
| 상대적 성능         | 1       | 3-4x V100    | 11x V100 | 18x V100 |
| 메모리              | HBM2    | HBM2e        | HBM3     | HBM3e    |
| 메모리 크기         | 16GB    | 40GB or 80GB | 80GB     | 141GB    |
| 메모리 Bandwidth    | 900GB/s | 1.5TB/s      | 3TB/s    | 4.8TB/s  |
| NVLink              | 25GB/s  | 50GB/s       | 100GB/s  | ?        |
| Infiniband 지원     | O       | O            | O        | O        |
| Tensor core 지원    | O       | O            | O        | O        |
| BF16 지원           | X       | O            | O        | O        |
| Flashattention 지원 | X       | O            | O        | O        |

## 상세 설명

- 상대적 성능
  - 성능은 여러 변수가 있고, gpu 개수에도 비례하기 때문에 정확하게 계산하기는 어렵습니다. 다만 llama2 모델을 사용한 경우 실험적으로 gpu개수에 비례적으로 추론 시간이 감소하였습니다 (즉, 1대에 10초 걸리면 2대 사용시에는 5초대)
- 메모리 크기
  - 메모리 크기도 사실 gpu 여러대로 커버가 가능한 부분이긴 하지만, gpu가 부족한 경우 모델의 크기를 감안해서 사용해야합니다. 요즘 모델들은 친절하게 파라미터 수를 모델 마지막에 제시하고 있고, quantization 옵션을 통해 파라미터를 8bit, 16bit 등등의 메모리 사용량이 적은 형태로 로드할 수 있게 도와줍니다. 대략 계산시에 8bit 기준으로 7B 모델의 경우 메모리 점유율이 7GB로 보시면 됩니다. 16bit의 경우 14GB이므로 V100에 간신히 올리지만, fp32의 경우 28GB이므로 V100 하나에는 못 올리게 됩니다.
- 메모리 Bandwidth
  - 데이터가 메모리에서 코어로의 이동량을 의미하는데, 이게 낮으면 코어가 idle 상태에 놓이는 경우가 많아서 추론 시간에 중대한 영향을 미칩니다.
- nvlink
  - 이것은 같은 노드내 nvidia gpu간의 통신에 사용되는 것으로, 여러 gpu를 사용할때 중요한 값입니다. 당연히 높을수록 gpu간 데이터 전송이 빠릅니다.
- infiniband
  - 이것은 다른 노드 간의 통신에 사용되는 것으로, 거대한 gpu 클러스터를 사용할 때 중요한 값입니다. 최근에 본 논문 중에 transfomer 모형의 decoder 과정의 단계를 초기 Q,K,V 계산하는 prompt computation과 이후 token generate 2단계로 나누어, 계산이 많은 첫번째 단계와 KV 캐시 기반의 메모리 사용이 많은 두번째 단계에 사용하는 gpu를 다르게 하고, 상태를 서로 전송하는 방식으로 속도 개선을 이뤘다는 논문이 있었는데, 이때 상태 전송에 infiniband가 사용됩니다.
- tensor core
  - matrix 연산이 많은 AI 모델들에 특화된 코어로써 v100에서 처음 선보여서 지속적으로 개선이 이뤄지고 있습니다. 특히 A100이후에는 sparse matrix 계산에 특화된 tensor core를 제공하고 있습니다.
- bf16
  - 자리수를 대표하는 지수항에 fp32와 동일하게 8bit를 할당함으로써 수의 정밀도를 포기함으로써 표현 가능한 숫자 범위를 확대한 방법인데, layer와 파라미터가 많은 transformer 모형은 곱을 하다보면 값이 크게 계산되기 때문에 메모리 사용량을 줄이면서 유용하게 쓸 수 있는 타입인데, 안타깝게도 v100에서는 못 씁니다.. pytorch에서는 torch.cuda.is_bf16_supported() 을 통해 지원 여부를 확인할 수 있습니다.
- flash attention
  - 현재 버전2까지 나왔는데, KV 캐싱 기반의 token generation이 많은 gpt형태의 decoder 기반 모델에서 gpu 자원의 효과적인 활용을 바탕으로 계산 속도를 획기적으로 올린 방식입니다. v100에서는 지원되지 않습니다.

## 결론

앞서 말씀드렸듯이, 돈 많고 최신 gpu가 많으면 정말 쓸때없는 고민의 글입니다.. 하지만 돈이 없을수도 있고, gpu를 못 구하는 요즘 시대이니 옛것이라도 잘 써보자는 취지로 봐주시면 되겠습니다.... 점점 모델들이 커지면서 이제 단순히 gpu 1대만으로 성능 최적화를 하기에는 힘든 시대이고, gpu 클러스터의 최적화를 통해 성능을 향상시키는 쪽으로 접근해야된다고 생각이 듭니다. 이런 점에서 기존의 gpu의 스펙과 한계를 파악해서 클러스터 내에서 최적의 역할을 수행할 수 있도록 설계하는 것도 중요하다고 생각됩니다.
